{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Artyom35689/Ad_bot/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "038aa900",
      "metadata": {
        "id": "038aa900"
      },
      "outputs": [],
      "source": [
        "import torch, sys, platform\n",
        "import torch.nn as nn\n",
        "import os, json, random, math, cv2, torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO val2017 (≈1 ГБ) + аннотации\n",
        "!mkdir -p data/coco && cd data/coco\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip -q val2017.zip\n",
        "!unzip -q annotations_trainval2017.zip\n",
        "!rm -f val2017.zip annotations_trainval2017.zip\n",
        "!cd ../../\n"
      ],
      "metadata": {
        "id": "o9JpkC2t8ygY",
        "outputId": "471c8aeb-fa92-4d4b-b39c-181f407fb673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o9JpkC2t8ygY",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 14:43:11--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.128.201, 16.182.70.49, 3.5.29.183, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.128.201|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip          26%[====>               ] 204.03M  55.3MB/s    eta 13s    ^C\n",
            "--2025-10-07 14:43:15--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.128.201, 16.182.70.49, 3.5.29.183, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.128.201|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "    annotations_tra  20%[===>                ]  49.01M  34.8MB/s               ^C\n",
            "[val2017.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of val2017.zip or\n",
            "        val2017.zip.zip, and cannot find val2017.zip.ZIP, period.\n",
            "[annotations_trainval2017.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of annotations_trainval2017.zip or\n",
            "        annotations_trainval2017.zip.zip, and cannot find annotations_trainval2017.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "52db958c",
      "metadata": {
        "id": "52db958c"
      },
      "outputs": [],
      "source": [
        "def layer(input,output,kernel,stride=1,padding=None):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(input,output,kernel,stride,padding if padding is not None else kernel//2,bias=False),\n",
        "            nn.BatchNorm2d(output),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "\n",
        "class YOLOv1(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=1):\n",
        "        super(YOLOv1, self).__init__()\n",
        "        self.S = S  # Grid size\n",
        "        self.B = B  # Number of bounding boxes per grid cell\n",
        "        self.C = C  # Number of classes, we will use 1 for person detection\n",
        "\n",
        "        L =[]\n",
        "        L+=[layer(3,64,7,2,3),  nn.MaxPool2d(2,2)]\n",
        "        L+=[layer(64,192,3),    nn.MaxPool2d(2,2)]\n",
        "        L+=[layer(192,128,1),   layer(128,256,3),       layer(256,256,1), layer(256,512,3),  nn.MaxPool2d(2,2)]\n",
        "        L+=[layer(512,256,1),   layer(256,512,3)]*4 + [ layer(512,512,1), layer(512,1024,3), nn.MaxPool2d(2,2)]\n",
        "        L+=[layer(1024,512,1),  layer(512,1024,3)]*2\n",
        "        L+=[layer(1024,1024,3), layer(1024,1024,3,2,1)]\n",
        "        L+=[layer(1024,1024,3), layer(1024,1024,3)]\n",
        "        self.backbone = nn.Sequential(*L)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, S * S * (C + B * 5))\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.fc(x)\n",
        "        # [batch_size, S*S*(C+5*B)] -> [batch_size, S, S, C+5*B]\n",
        "        # [C+5*B] = [x,y,w,h,conf1,conf2,...,confB,class1,class2,...,classC]\n",
        "        x = x.view(-1, self.S, self.S, self.C + self.B * 5)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _iou_xywh(a, b, eps=1e-9):\n",
        "    # a,b: [...,4] in cx,cy,w,h (abs in [0,1])\n",
        "    a_x1 = a[...,0] - a[...,2]/2; a_y1 = a[...,1] - a[...,3]/2\n",
        "    a_x2 = a[...,0] + a[...,2]/2; a_y2 = a[...,1] + a[...,3]/2\n",
        "    b_x1 = b[...,0] - b[...,2]/2; b_y1 = b[...,1] - b[...,3]/2\n",
        "    b_x2 = b[...,0] + b[...,2]/2; b_y2 = b[...,1] + b[...,3]/2\n",
        "    inter = (torch.minimum(a_x2,b_x2)-torch.maximum(a_x1,b_x1)).clamp_(min=0) * \\\n",
        "            (torch.minimum(a_y2,b_y2)-torch.maximum(a_y1,b_y1)).clamp_(min=0)\n",
        "    area_a = (a_x2-a_x1).clamp(min=0)*(a_y2-a_y1).clamp(min=0)\n",
        "    area_b = (b_x2-b_x1).clamp(min=0)*(b_y2-b_y1).clamp(min=0)\n",
        "    return inter / (area_a + area_b - inter + eps)\n",
        "\n",
        "class YoloV1Loss(nn.Module):\n",
        "    def __init__(self,S=7,B=2,lambda_coord=5.0,lambda_noobj=0.5):\n",
        "        super().__init__()\n",
        "        self.S,self.B = S,B\n",
        "        self.lc, self.lno = lambda_coord, lambda_noobj\n",
        "\n",
        "    def forward(self,pred,target):\n",
        "        \"\"\"\n",
        "        pred: [N,S,S,B*5+1]\n",
        "        target: dict(tconf[N,S,S,1], txywh[N,S,S,4], tcls[N,S,S,1])\n",
        "        \"\"\"\n",
        "        N,S,B = pred.size(0), self.S, self.B\n",
        "        device = pred.device\n",
        "\n",
        "        pb = pred[...,:B*5].view(N,S,S,B,5)   # x,y,sw,sh,conf (sw,sh ~ sqrt(w,h))\n",
        "        pcls = pred[...,B*5:]                 # one-class logit\n",
        "\n",
        "        px = pb[...,0].sigmoid()\n",
        "        py = pb[...,1].sigmoid()\n",
        "        # стабильные толщины: softplus ≥0\n",
        "        sw = torch.nn.functional.softplus(pb[...,2]).clamp(max=10.0)\n",
        "        sh = torch.nn.functional.softplus(pb[...,3]).clamp(max=10.0)\n",
        "        pc = pb[...,4].sigmoid()\n",
        "        pcl = pcls.sigmoid()\n",
        "\n",
        "        # абсолютные боксы\n",
        "        gy, gx = torch.meshgrid(torch.arange(S, device=device),\n",
        "                                torch.arange(S, device=device), indexing='ij')\n",
        "        gx = gx.view(1,S,S,1).float(); gy = gy.view(1,S,S,1).float()\n",
        "        bx = (gx + px)/S\n",
        "        by = (gy + py)/S\n",
        "        bw = (sw ** 2).clamp(min=1e-9, max=1.0)\n",
        "        bh = (sh ** 2).clamp(min=1e-9, max=1.0)\n",
        "        boxes_abs = torch.stack([bx,by,bw,bh], dim=-1)  # [N,S,S,B,4]\n",
        "\n",
        "        # таргеты\n",
        "        tconf = target['tconf'].to(device)     # [N,S,S,1]\n",
        "        txywh = target['txywh'].to(device)     # [N,S,S,4]\n",
        "        tcls  = target['tcls' ].to(device)     # [N,S,S,1]\n",
        "\n",
        "        # выбор ответственного бокса\n",
        "        ious = _iou_xywh(boxes_abs, txywh.unsqueeze(3).expand_as(boxes_abs))  # [N,S,S,B]\n",
        "        iou_max, argmax = ious.max(dim=3, keepdim=True)\n",
        "        obj_mask = tconf                                      # [N,S,S,1]\n",
        "        resp = torch.zeros_like(ious)\n",
        "        resp.scatter_(3, argmax, 1.0)\n",
        "        resp = resp * obj_mask                                # [N,S,S,B]\n",
        "\n",
        "        # координатный терм в \"sqrt-пространстве\"\n",
        "        pxr = (px*resp).sum(dim=3, keepdim=True)\n",
        "        pyr = (py*resp).sum(dim=3, keepdim=True)\n",
        "        swr = (sw*resp).sum(dim=3, keepdim=True)\n",
        "        shr = (sh*resp).sum(dim=3, keepdim=True)\n",
        "\n",
        "        tx,ty,tw,th = txywh.unbind(-1)     # [N,S,S]\n",
        "        tx = tx.unsqueeze(-1) * S - gx\n",
        "        ty = ty.unsqueeze(-1) * S - gy\n",
        "        tsw = (tw.unsqueeze(-1).clamp(min=1e-9)).sqrt()\n",
        "        tsh = (th.unsqueeze(-1).clamp(min=1e-9)).sqrt()\n",
        "\n",
        "        coord = ((pxr - tx)**2 + (pyr - ty)**2 +\n",
        "                 (swr - tsw)**2 + (shr - tsh)**2)\n",
        "        coord = self.lc * (coord * obj_mask).sum()\n",
        "\n",
        "        # confidence: к IoU для ответственного, 0 для остальных\n",
        "        conf_tgt_resp = (ious * resp).detach()\n",
        "        conf_obj   = ((pc - conf_tgt_resp)**2 * resp).sum()\n",
        "        conf_noobj = self.lno * ((pc**2) * (1.0 - resp)).sum()\n",
        "        conf = conf_obj + conf_noobj\n",
        "\n",
        "        # class (C=1)\n",
        "        cls = ((pcl - tcls)**2 * obj_mask).sum()\n",
        "\n",
        "        total = coord + conf + cls\n",
        "        total = torch.nan_to_num(total, nan=0.0, posinf=1e9, neginf=0.0) / max(N,1)\n",
        "\n",
        "        logs = {\n",
        "            'loss': float(total.detach().cpu()),\n",
        "            'L_coord': float((coord/max(N,1)).detach().cpu()),\n",
        "            'L_conf_obj': float((conf_obj/max(N,1)).detach().cpu()),\n",
        "            'L_conf_noobj': float((conf_noobj/max(N,1)).detach().cpu()),\n",
        "            'L_cls': float((cls/max(N,1)).detach().cpu())\n",
        "        }\n",
        "        # простые метрики\n",
        "        with torch.no_grad():\n",
        "            any_conf = (pc > 0.5).any(dim=3, keepdim=True).float()  # [N,S,S,1]\n",
        "            obj_acc = float((any_conf == tconf).float().mean().cpu())\n",
        "            mean_iou = float(((iou_max * obj_mask).sum() / (obj_mask.sum().clamp(min=1))).detach().cpu())\n",
        "            logs['obj_acc'] = obj_acc\n",
        "            logs['mean_iou'] = mean_iou\n",
        "        return total, logs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdCtbffG7ok4"
      },
      "id": "ZdCtbffG7ok4",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S, B = 7, 2\n",
        "IM = 448\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "class CocoPersonMini(Dataset):\n",
        "    def __init__(self, root=\"\", limit=1000):\n",
        "        self.img_dir = os.path.join(root,\"val2017\")\n",
        "        ann = json.load(open(os.path.join(root,\"annotations\",\"instances_val2017.json\")))\n",
        "        pid=1\n",
        "        anns_by={}\n",
        "        for a in ann[\"annotations\"]:\n",
        "            if a[\"category_id\"]!=pid: continue\n",
        "            iid=a[\"image_id\"]; x,y,w,h=a[\"bbox\"]\n",
        "            if w<=1 or h<=1: continue\n",
        "            anns_by.setdefault(iid, []).append((x,y,w,h))\n",
        "        id2img={im[\"id\"]:im for im in ann[\"images\"]}\n",
        "        imgs=[(iid,id2img[iid]) for iid in anns_by.keys()]\n",
        "        random.seed(0); random.shuffle(imgs); imgs=imgs[:limit]\n",
        "        self.items=[]\n",
        "        for iid,meta in imgs:\n",
        "            path=os.path.join(self.img_dir, meta[\"file_name\"])\n",
        "            if not os.path.exists(path): continue\n",
        "            H,W=meta[\"height\"],meta[\"width\"]\n",
        "            boxes=[]\n",
        "            for (x,y,w,h) in anns_by[iid]:\n",
        "                cx=(x+w/2)/W; cy=(y+h/2)/H; bw=w/W; bh=h/H\n",
        "                if 0<cx<1 and 0<cy<1 and bw>0 and bh>0:\n",
        "                    boxes.append([cx,cy,bw,bh])\n",
        "            if boxes: self.items.append((path,boxes))\n",
        "        print(\"COCO val2017 person:\", len(self.items))\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        path,boxes=self.items[i]\n",
        "        img0=cv2.imread(path)\n",
        "        if img0 is None: raise FileNotFoundError(path)\n",
        "        img0=cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
        "        H0,W0=img0.shape[:2]\n",
        "        scale=min(IM/W0, IM/H0)\n",
        "        nw,nh=int(W0*scale+0.5),int(H0*scale+0.5)\n",
        "        img=cv2.resize(img0,(nw,nh), interpolation=cv2.INTER_LINEAR)\n",
        "        canvas=np.zeros((IM,IM,3),np.uint8)\n",
        "        top=(IM-nh)//2; left=(IM-nw)//2\n",
        "        canvas[top:top+nh,left:left+nw]=img\n",
        "\n",
        "        adj=[]\n",
        "        for (cx,cy,bw,bh) in boxes:\n",
        "            cx_n=(cx*W0*scale + left)/IM\n",
        "            cy_n=(cy*H0*scale + top)/IM\n",
        "            bw_n=(bw*W0*scale)/IM\n",
        "            bh_n=(bh*H0*scale)/IM\n",
        "            if 0<cx_n<1 and 0<cy_n<1 and bw_n>0 and bh_n>0:\n",
        "                adj.append([cx_n,cy_n,bw_n,bh_n])\n",
        "\n",
        "        img_t=torch.from_numpy(canvas).permute(2,0,1).float()/255.0\n",
        "        tconf=torch.zeros((S,S,1),dtype=torch.float32)\n",
        "        txywh=torch.zeros((S,S,4),dtype=torch.float32)\n",
        "        tcls =torch.zeros((S,S,1),dtype=torch.float32)\n",
        "        occ=set()\n",
        "        for (cx,cy,bw,bh) in adj:\n",
        "            gx=min(S-1,int(cx*S)); gy=min(S-1,int(cy*S))\n",
        "            if (gy,gx) in occ: continue  # 1 объект на клетку\n",
        "            occ.add((gy,gx))\n",
        "            tconf[gy,gx,0]=1.0\n",
        "            txywh[gy,gx,:]=torch.tensor([cx,cy,bw,bh])\n",
        "            tcls[gy,gx,0]=1.0\n",
        "        return img_t, {'tconf':tconf,'txywh':txywh,'tcls':tcls}\n",
        "\n",
        "def collate(batch):\n",
        "    imgs,T=zip(*batch)\n",
        "    imgs=torch.stack(imgs,0)\n",
        "    out={'tconf':torch.stack([t['tconf'] for t in T],0),\n",
        "         'txywh':torch.stack([t['txywh'] for t in T],0),\n",
        "         'tcls' :torch.stack([t['tcls']  for t in T],0)}\n",
        "    return imgs,out\n",
        "\n",
        "# ---------- ТРЕНИРОВКА ----------\n",
        "def one_epoch(model, loss_fn, loader, opt, device):\n",
        "    model.train()\n",
        "    avg = {'loss':0,'L_coord':0,'L_conf_obj':0,'L_conf_noobj':0,'L_cls':0,'obj_acc':0,'mean_iou':0}\n",
        "    n=0\n",
        "    for imgs, tgt in tqdm(loader, ncols=80):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        tgt  = {k:v.to(device, non_blocking=True) for k,v in tgt.items()}\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        pred = model(imgs)\n",
        "        loss, logs = loss_fn(pred, tgt)\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        opt.step()\n",
        "        for k in avg: avg[k]+=logs.get(k,0.0)\n",
        "        n+=1\n",
        "    for k in avg: avg[k]/=max(n,1)\n",
        "    return avg\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    ds = CocoPersonMini(limit=1000)\n",
        "    dl = DataLoader(ds, batch_size=4, shuffle=True, num_workers=2,\n",
        "                    pin_memory=True, persistent_workers=False, collate_fn=collate)\n",
        "\n",
        "    model = YOLOv1(S=7,B=2,C=1).to(device)\n",
        "    loss_fn = YoloV1Loss(S=7,B=2,lambda_coord=5.0,lambda_noobj=0.5)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=3e-3, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    logs = one_epoch(model, loss_fn, dl, opt, device)\n",
        "    print(logs)\n",
        "    # torch.save(model.state_dict(), \"yolov1_person_1epoch.pth\")"
      ],
      "metadata": {
        "id": "FCqzk5iW7pmX",
        "outputId": "0ec5e1f6-7f62-4260-8b99-595e2efa1ace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FCqzk5iW7pmX",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO val2017 person: 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████| 250/250 [00:53<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 7.581509563446045, 'L_coord': 4.578233995437622, 'L_conf_obj': 0.14163775842264295, 'L_conf_noobj': 2.4916467794179917, 'L_cls': 0.36999103239923714, 'obj_acc': 0.8837346683740616, 'mean_iou': 0.20877978977560996}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
